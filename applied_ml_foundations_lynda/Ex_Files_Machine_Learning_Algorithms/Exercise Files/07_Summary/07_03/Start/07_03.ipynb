{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Compare model results and final model selection\n",
    "\n",
    "Using the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition.\n",
    "\n",
    "In this section, we will do the following:\n",
    "1. Evaluate all of our saved models on the validation set\n",
    "2. Select the best model based on performance on the validation set\n",
    "3. Evaluate that model on the holdout test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from time import time\n",
    "\n",
    "val_features = pd.read_csv('../../../val_features.csv')\n",
    "val_labels = pd.read_csv('../../../val_labels.csv' )\n",
    "\n",
    "te_features = pd.read_csv('../../../test_features.csv')\n",
    "te_labels = pd.read_csv('../../../test_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\robk\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator SVC from version 0.20.1 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "\n",
    "for mdl in ['LR', 'SVM', 'MLP', 'RF', 'GB']:\n",
    "    models[mdl] = joblib.load('../../../{}_model.pkl'.format(mdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\robk\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'LR': LogisticRegression(C=0.1),\n",
       " 'SVM': SVC(C=0.1, break_ties=None, gamma='auto_deprecated', kernel='linear'),\n",
       " 'MLP': MLPClassifier(learning_rate='invscaling'),\n",
       " 'RF': RandomForestClassifier(max_depth=4, n_estimators=50),\n",
       " 'GB': GradientBoostingClassifier(learning_rate=0.01, n_estimators=500)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models on the validation set\n",
    "\n",
    "![Evaluation Metrics](../../img/eval_metrics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, features, labels):\n",
    "    start = time()\n",
    "    \n",
    "    # predict\n",
    "    predictions = model.predict(features)\n",
    "    \n",
    "    end = time()\n",
    "    accuracy = round(accuracy_score(labels, predictions), 3)\n",
    "    precision = round(precision_score(labels, predictions), 3)\n",
    "    recall = round(recall_score(labels, predictions), 3)\n",
    "    print('{} -- Accuracy: {} / Precision: {} / Recall: {} / Latency: {}ms'.format(name,\n",
    "                                                                                   accuracy,\n",
    "                                                                                   precision,\n",
    "                                                                                   recall,\n",
    "                                                                                   round((end - start)*1000, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR -- Accuracy: 0.659 / Precision: 0.826 / Recall: 0.25 / Latency: 1.0ms\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SVC' object has no attribute 'break_ties'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-678d5663854f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmdl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mevaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-6cc1b23a941d>\u001b[0m in \u001b[0;36mevaluate_model\u001b[1;34m(name, model, features, labels)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\robk\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    604\u001b[0m         \"\"\"\n\u001b[0;32m    605\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbreak_ties\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function_shape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'ovo'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m             raise ValueError(\"break_ties must be False when \"\n\u001b[0;32m    608\u001b[0m                              \"decision_function_shape is 'ovo'\")\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SVC' object has no attribute 'break_ties'"
     ]
    }
   ],
   "source": [
    "for name, mdl in models.items():\n",
    "    evaluate_model(name, mdl, val_features, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR -- Accuracy: 0.691 / Precision: 0.75 / Recall: 0.231 / Latency: 1.0ms\n",
    "\n",
    "SVM -- Accuracy: 0.747 / Precision: 0.672 / Recall: 0.6 / Latency: 2.0ms\n",
    "\n",
    "MLP -- Accuracy: 0.382 / Precision: 0.368 / Recall: 0.969 / Latency: 1.0ms\n",
    "\n",
    "RF -- Accuracy: 0.798 / Precision: 0.774 / Recall: 0.631 / Latency: 5.0ms\n",
    "\n",
    "GB -- Accuracy: 0.815 / Precision: 0.808 / Recall: 0.646 / Latency: 3.0ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate best model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost -- Accuracy: 0.815 / Precision: 0.808 / Recall: 0.646 / Latency: 2.0ms\n",
      "Random Forest -- Accuracy: 0.798 / Precision: 0.774 / Recall: 0.631 / Latency: 6.0ms\n"
     ]
    }
   ],
   "source": [
    "evaluate_model('Gradient Boost', models['GB'], te_features, te_labels)\n",
    "evaluate_model('Random Forest', models['RF'], te_features, te_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
