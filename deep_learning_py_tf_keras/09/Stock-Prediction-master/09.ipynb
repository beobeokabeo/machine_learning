{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bittrackedtfgpuconda0f6711e19b92447eb6b3c3c33c92d32d",
   "display_name": "Python 3.7.7 64-bit ('tracked-tf-gpu': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of missing values: 6\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "comass, span, halflife, and alpha are mutually exclusive",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-213833071041>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChaikin_osc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;31m# Chakin Volatility indicator (30, 40, 50 Days)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-213833071041>\u001b[0m in \u001b[0;36mChaikin_osc\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mChaikin_osc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[0mad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Close'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'High'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Low'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'High'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Low'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Volume'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m     \u001b[0mChaikin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mewm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_periods\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mewm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_periods\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Chaikin_osc(3,10)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mChaikin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tracked-tf-gpu\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mewm\u001b[1;34m(self, com, span, halflife, alpha, min_periods, adjust, ignore_na, axis, times)\u001b[0m\n\u001b[0;32m  10583\u001b[0m                 \u001b[0mignore_na\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_na\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10584\u001b[0m                 \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10585\u001b[1;33m                 \u001b[0mtimes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  10586\u001b[0m             )\n\u001b[0;32m  10587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tracked-tf-gpu\\lib\\site-packages\\pandas\\core\\window\\ewm.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, obj, com, span, halflife, alpha, min_periods, adjust, ignore_na, axis, times)\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhalflife\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_center_of_mass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhalflife\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tracked-tf-gpu\\lib\\site-packages\\pandas\\core\\window\\ewm.py\u001b[0m in \u001b[0;36mget_center_of_mass\u001b[1;34m(comass, span, halflife, alpha)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mvalid_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_not_none\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhalflife\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalid_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"comass, span, halflife, and alpha are mutually exclusive\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# Convert to center of mass; domain checks ensure 0 < alpha <= 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: comass, span, halflife, and alpha are mutually exclusive"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu April 26 09:22:31 2018\n",
    "@author: Faris Mismar\n",
    "\"\"\"\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas.core.window import ewm, expanding, rolling\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#os.chdir('/Users/farismismar/Dropbox/Stock Trading Using ML')\n",
    "\n",
    "# Check if tensorflow is used\n",
    "if (keras.backend.backend() != 'tensorflow' and keras.backend.image_data_format() != 'channels_last' and keras.backend.image_dim_ordering() != 'tf'):\n",
    "    print('Install tensorflow, configure keras.json to include channels_last for image format and tf for image dimension ordering.')\n",
    "    print('Program will now exit.')\n",
    "    sys.exit(1)\n",
    "    \n",
    "# Set the random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Import the datafile to memory first\n",
    "TICKER = 'YESBANK.NS'\n",
    "\n",
    "dataset = pd.read_csv('./Dataset/{}.csv'.format(TICKER))\n",
    "\n",
    "# Sanity check. Missing values?\n",
    "print('Number of missing values: {}'.format(dataset.isnull().sum().sum()))\n",
    "\n",
    "# Generate more features X for the data\n",
    "# Ask 1: \n",
    "# Simple Moving Average (30, 40, 50 Days) for closing prices\n",
    "# Reference: https://www.investopedia.com/articles/active-trading/052014/how-use-moving-average-buy-stocks.asp\n",
    "for i in np.array([30,40,50]):\n",
    "    sma_i = dataset.rolling(window=i).mean()\n",
    "    sma_i = dataset[['Close']]\n",
    "    # Change the column name\n",
    "    sma_i = sma_i.rename(index=str,columns={'Close': 'Close_SMA_{}'.format(i)})\n",
    "    # Append it as a new feature\n",
    "    dataset = dataset.join(sma_i)\n",
    "\n",
    "# Ask 2: \n",
    "# Exponential Moving Average (30, 40, 50 Days) for closing prices\n",
    "# Reference: https://www.investopedia.com/articles/active-trading/052014/how-use-moving-average-buy-stocks.asp\n",
    "for i in np.array([30,40,50]):\n",
    "    ema_i = pd.Series.ewm(dataset['Close'], span=i).mean().to_frame()\n",
    "    # Change the column name\n",
    "    ema_i = ema_i.rename(index=str,columns={'Close': 'Close_EMA_{}'.format(i)})\n",
    "    # Append it as a new feature\n",
    "    dataset = dataset.join(ema_i)\n",
    "\n",
    "# Ask 3:\n",
    "# Aroon Oscillator ( 30, 40, 50 Days) for TBD?\n",
    "# Reference: TBD\n",
    "def aroon(df, n=25):\n",
    "    up = 100 * df['High'].rolling(window=n +1).apply(lambda x: x.argmax()) / n # * pd.rolling_apply(df['High'], n + 1, lambda x: x.argmax()) / n\n",
    "    dn = 100 * df['Low'].rolling(window=n+1).apply(lambda x: x.argmin())  / n  # pd.rolling_apply(df['Low'], n + 1, lambda x: x.argmin()) / n\n",
    "    return pd.DataFrame(dict(up=up, down=dn))\n",
    "\n",
    "for i in np.array([30,40,50]):\n",
    "    aroon_i = aroon(dataset, i)\n",
    "    aroon_osc_i = aroon_i['down'] - aroon_i['up']\n",
    "    aroon_osc_i = aroon_osc_i.to_frame()\n",
    "    aroon_osc_i.columns = ['aroon_osc_{}'.format(i)]\n",
    "    # Append it as a new feature\n",
    "    dataset = dataset.join(aroon_osc_i)\n",
    "\n",
    "# Ask 4:\n",
    "# MACD\n",
    "# Reference: https://github.com/Crypto-toolbox/pandas-technical-indicators/blob/master/technical_indicators.py\n",
    "def macd(df, n_fast=26, n_slow=12):\n",
    "    \"\"\"Calculate MACD, MACD Signal and MACD difference\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n_fast: \n",
    "    :param n_slow: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    EMAfast = pd.Series(df['Close'].ewm(span=n_fast, min_periods=n_slow).mean())\n",
    "    EMAslow = pd.Series(df['Close'].ewm(span=n_slow, min_periods=n_slow).mean())\n",
    "    MACD = pd.Series(EMAfast - EMAslow, name='MACD_' + str(n_fast) + '_' + str(n_slow))\n",
    "    MACDsign = pd.Series(MACD.ewm(span=9, min_periods=9).mean(), name='MACDsign_' + str(n_fast) + '_' + str(n_slow))\n",
    "    MACDdiff = pd.Series(MACD - MACDsign, name='MACDdiff_' + str(n_fast) + '_' + str(n_slow))\n",
    "    df = df.join(MACD)\n",
    "    df = df.join(MACDsign)\n",
    "    df = df.join(MACDdiff)\n",
    "    return df\n",
    "\n",
    "dataset = macd(dataset)\n",
    "\n",
    "# Ask 5:\n",
    "# RSI\n",
    "# Reference: https://github.com/Crypto-toolbox/pandas-technical-indicators/blob/master/technical_indicators.py\n",
    "# Choice of n = 14 is due to Wilder recommended a smoothing period of 14 (see exponential smoothing, i.e. α = 1/14 or N = 14).\n",
    "def relative_strength_index(df, n=14):\n",
    "    \"\"\"Calculate Relative Strength Index(RSI) for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    UpI = [0]\n",
    "    DoI = [0]\n",
    "    while i + 1 <= df.index[-1]:\n",
    "        UpMove = df.loc[i + 1, 'High'] - df.loc[i, 'High']\n",
    "        DoMove = df.loc[i, 'Low'] - df.loc[i + 1, 'Low']\n",
    "        if UpMove > DoMove and UpMove > 0:\n",
    "            UpD = UpMove\n",
    "        else:\n",
    "            UpD = 0\n",
    "        UpI.append(UpD)\n",
    "        if DoMove > UpMove and DoMove > 0:\n",
    "            DoD = DoMove\n",
    "        else:\n",
    "            DoD = 0\n",
    "        DoI.append(DoD)\n",
    "        i = i + 1\n",
    "    UpI = pd.Series(UpI)\n",
    "    DoI = pd.Series(DoI)\n",
    "    PosDI = pd.Series(UpI.ewm(span=n, min_periods=n).mean())\n",
    "    NegDI = pd.Series(DoI.ewm(span=n, min_periods=n).mean())\n",
    "    RSI = pd.Series(PosDI / (PosDI + NegDI), name='RSI_' + str(n))\n",
    "    #df = df.join(RSI)\n",
    "    return RSI\n",
    "\n",
    "dataset = dataset.join(relative_strength_index(dataset))\n",
    "\n",
    "# Ask 6:\n",
    "# Bollinger Bands ( 30, 40, 50 Days)\n",
    "# Reference: https://github.com/Crypto-toolbox/pandas-technical-indicators/blob/master/technical_indicators.py\n",
    "def bollinger_bands(df, n):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    MA = pd.Series(df['Close'].rolling(window=n, min_periods=n).mean())\n",
    "    MSD = pd.Series(df['Close'].rolling(window=n, min_periods=n).std())\n",
    "    b1 = 4 * MSD / MA\n",
    "    B1 = pd.Series(b1, name='BollingerB_' + str(n))\n",
    "    df = df.join(B1)\n",
    "    b2 = (df['Close'] - MA + 2 * MSD) / (4 * MSD)\n",
    "    B2 = pd.Series(b2, name='Bollinger%b_' + str(n))\n",
    "    df = df.join(B2)\n",
    "    return df\n",
    "\n",
    "for i in np.array([30,40,50]):\n",
    "    dataset = bollinger_bands(dataset, i)\n",
    "\t\n",
    "# Ask 7:\n",
    "# Stochastic Oscillator (d and k)\n",
    "def stochastic_oscillator_d(df, n):\n",
    "    \"\"\"Calculate stochastic oscillator %D for given data.\n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    SOk = pd.Series((df['Close'] - df['Low']) / (df['High'] - df['Low']), name='SO%k')\n",
    "    SOd = pd.Series(SOk.ewm(span=n, min_periods=n).mean(), name='SO%d_' + str(n))\n",
    "    df = df.join(SOd)\n",
    "    return df\n",
    "\n",
    "def stochastic_oscillator_k(df):\n",
    "    \"\"\"Calculate stochastic oscillator %K for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    SOk = pd.Series((df['Close'] - df['Low']) / (df['High'] - df['Low']), name='SO%k')\n",
    "    df = df.join(SOk)\n",
    "    return df\n",
    "\n",
    "dataset = stochastic_oscillator_k(dataset)\n",
    "\n",
    "for i in np.array([5, 9, 14]):\n",
    "    dataset = stochastic_oscillator_d(dataset, i)\n",
    "\t\t\n",
    "# Ask 8:\n",
    "# Stochastic momentum Indicator\n",
    "# https://github.com/kylejusticemagnuson/pyti/blob/master/pyti/stochrsi.py\n",
    "def stochastic_momentum_ind(df, n):\n",
    "    \"\"\"\n",
    "    StochRSI.\n",
    "    Formula:\n",
    "    SRSI = ((RSIt - RSI LOW) / (RSI HIGH - LOW RSI)) * 100\n",
    "    \"\"\"\n",
    "    rsi = relative_strength_index(df, n)[n:]\n",
    "    stochrsi = [100 * ((rsi[idx] - np.min(rsi[idx+1-n:idx+1])) / (np.max(rsi[idx+1-n:idx+1]) - np.min(rsi[idx+1-n:idx+1]))) for idx in range(n-1, len(rsi))]\n",
    "    #stochrsi = fill_for_noncomputable_vals(data, stochrsi)\n",
    "    return stochrsi\n",
    "'''\n",
    "    aa = pd.DataFrame(data=np.array([12,13,14,15,12,11,10,9,8,7,10,12,14,16]))\n",
    "    aa.rolling(window=3).apply(lambda x:x[2]-x[0])\n",
    "    '''\n",
    "# Ask 9:\n",
    "# Chande Momentum Oscillator\n",
    "# Reference: https://www.investopedia.com/terms/c/chandemomentumoscillator.asp\n",
    "# https://github.com/kylejusticemagnuson/pyti/tree/master/pyti\n",
    "def chande_momentum_oscillator(df, n):\n",
    "    \"\"\"\n",
    "    Chande Momentum Oscillator.\n",
    "    Formula:\n",
    "    cmo = 100 * ((sum_up - sum_down) / (sum_up + sum_down))\n",
    "    \"\"\"\n",
    "\n",
    "    close_data = np.array(df['Close'])\n",
    "\n",
    "    moving_period_diffs = [[(close_data[idx+1-n:idx+1][i] -\n",
    "                 close_data[idx+1-n:idx+1][i-1]) for i in range(1, len(close_data[idx+1-n:idx+1]))] for idx in range(0, len(close_data))]\n",
    "\n",
    "    sum_up = []\n",
    "    sum_down = []\n",
    "    for period_diffs in moving_period_diffs:\n",
    "        ups = [val if val > 0 else 0 for val in period_diffs]\n",
    "        sum_up.append(sum(ups))\n",
    "        downs = [abs(val) if val < 0 else 0 for val in period_diffs]\n",
    "        sum_down.append(sum(downs))\n",
    "\n",
    "    sum_up = np.array(sum_up)\n",
    "    sum_down = np.array(sum_down)\n",
    "\n",
    "    cmo = pd.Series(100 * ((sum_up - sum_down) / (sum_up + sum_down)), name='Chande_'+str(n))\n",
    "    df = df.join(cmo)\n",
    "    return df\n",
    "\n",
    "# Ask 10:\n",
    "# Commodity Channel Index (30, 40, 50 Days)\n",
    "def commodity_channel_index(df, n):\n",
    "    \"\"\"Calculate Commodity Channel Index for given data.\n",
    "\n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    PP = (df['High'] + df['Low'] + df['Close']) / 3\n",
    "    CCI = pd.Series((PP - PP.rolling(window=n, min_periods=n).mean()) / PP.rolling(window=n, min_periods=n).std(), name='CCI_' + str(n))\n",
    "    df = df.join(CCI)\n",
    "    return df\n",
    "\t\n",
    "for i in np.array([30,40,50]):\n",
    "    dataset = commodity_channel_index(dataset, i)\n",
    "\n",
    "# Ask 11:\n",
    "# Reference: https://www.quantopian.com/posts/technical-analysis-indicators-without-talib-code\n",
    "# Proof of magnitude is: http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:chaikin_oscillator\n",
    "def Chaikin_osc(df):  \n",
    "    ad = (2 * df['Close'] - df['High'] - df['Low']) / (df['High'] - df['Low']) * df['Volume']  \n",
    "    Chaikin = pd.Series(df.ewm(ad, span = 3, min_periods = 3) - df.ewm(ad, span = 10, min_periods = 10), name = 'Chaikin_osc(3,10)')\n",
    "    df = df.join(Chaikin)\n",
    "\n",
    "    return df\n",
    "\n",
    "dataset = Chaikin_osc(dataset)\n",
    "\n",
    "# Chakin Volatility indicator (30, 40, 50 Days)\n",
    "# Reference: https://www.linnsoft.com/techind/chaikin-money-flow-cmf\n",
    "def Chaikin(df,n):\n",
    "    AD = df['Volume']* (df['Close'] - df['Open']) / (df['High'] - df['Low']) # AD = VOL * (CL - OP) / (HI - LO) #AD stands for Accumulation Distribution\n",
    "    CMF = pd.Series(AD.rolling(window=n, min_periods=n).sum()) / pd.Series(df['Volume'].rolling(window=n, min_periods=n).sum()) #    CMF = SUM(AD, n) / SUM(VOL, n) where n = Period\n",
    "    Chaikin = pd.Series(CMF, name='Chaikin_'+str(n))\n",
    "    Chaikin_dir = pd.Series(Chaikin > 0, name='Chaikin_dir_'+str(n)) + 0\n",
    "    df = df.join(Chaikin)\n",
    "    df = df.join(Chaikin_dir)\n",
    "    \n",
    "    return df\n",
    "\n",
    "for i in np.array([30,40,50]):\n",
    "    dataset = Chaikin(dataset, i)\n",
    "    \n",
    "# Ask 12:\n",
    "# Trend Detection Index (30, 40, 50 Days)\n",
    "# https://www.linnsoft.com/techind/trend-detection-index-tdi\n",
    "def trend_detection_index(df, n):     \n",
    "    '''\n",
    "    Mom = Price - Price[Period] \n",
    "    MomAbs = Abs(Mom) \n",
    "    MomSum = Sum(Mom, Period) \n",
    "    MomSumAbs = Abs(MomSum) \n",
    "    MomAbsSum = Sum(MomAbs, Period) \n",
    "    MomAbsSum2 = Sum(MomAbs, Period * 2) \n",
    "    TDI = MomSumAbs - (MomAbsSum2 - MomAbsSum)\n",
    "    '''\n",
    "    \n",
    "    Mom = [(df['Close'][idx-n] - df['Close'][idx]) for idx in range(n,len(df['Close']))]\n",
    "    MomAbs = pd.Series(Mom).abs()\n",
    "    MomSum = pd.Series(Mom).rolling(window=n, min_periods=n).sum()\n",
    "    MomSumAbs = MomSum.abs()\n",
    "\n",
    "    MomAbsSum = pd.Series(MomAbs).rolling(window=n, min_periods=n).sum()\n",
    "    MomAbsSum2 = pd.Series(MomAbs).rolling(window=2*n, min_periods=2*n).sum()\n",
    "    \n",
    "    TDI = pd.Series(MomSumAbs - (MomAbsSum2 - MomAbsSum), name='TDI_'+str(n))\n",
    "    \n",
    "    df = df.join(TDI)\n",
    "    \n",
    "    return df\n",
    "\n",
    "for i in np.array([30,40,50]):\n",
    "    dataset = trend_detection_index(dataset, i)\n",
    "    \n",
    "# Ask 13:\n",
    "# Rate of Price Change (30, 40, 50 Days)\n",
    "def rate_of_price_change(df, n):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    M = df['Close'].diff(n - 1)\n",
    "    N = df['Close'].shift(n - 1)\n",
    "    ROC = pd.Series(M / N, name='ROPC_' + str(n))\n",
    "    df = df.join(ROC)\n",
    "    return df\n",
    "\t\n",
    "for i in np.array([30,40,50]):\n",
    "    dataset = rate_of_price_change(dataset, i)\n",
    "\n",
    "# Ask 14:\n",
    "# Rate of Volume Change (30, 40, 50 Days)\n",
    "def rate_of_volume_change(df, n):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    M = df['Volume'].diff(n - 1)\n",
    "    N = df['Volume'].shift(n - 1)\n",
    "    ROC = pd.Series(M / (0.00001 + N), name='ROVC_' + str(n))\n",
    "    df = df.join(ROC)\n",
    "    return df\n",
    "\t\n",
    "for i in np.array([30,40,50]):\n",
    "    dataset = rate_of_volume_change(dataset, i)\n",
    "\n",
    "# Ask 15:\n",
    "# William %R (30, 40, 50 Days)\n",
    "# Code written after: https://tradingsim.com/blog/williams-percent-r/\n",
    "# and http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:williams_r\n",
    "def william_r(df, n):\n",
    "    \"\"\"Calculate William %R for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    high=df['High'].rolling(window=n).max()\n",
    "    low=df['Low'].rolling(window=n).min()\n",
    "    \n",
    "    william = pd.Series((high - df['Close']) / (high - low), name='William%R_' + str(n)) * -100.\n",
    "    df = df.join(william)\n",
    "    return df\n",
    "\n",
    "for i in np.array([30,40,50]):\n",
    "    dataset = william_r(dataset, i)\n",
    "    \n",
    "# Generate difference and lag data\n",
    "# https://machinelearningmastery.com/tune-lstm-hyperparameters-keras-time-series-forecasting/\n",
    "\n",
    "def difference(df, lag=1):\n",
    "    return df.diff(lag)\n",
    "\n",
    "def lag(df, l=1):    \n",
    "    return df.shift(l)\n",
    "\n",
    "############################################################################################\n",
    "# Save the file\n",
    "dataset.to_csv(path_or_buf='./Dataset/{}_complete.csv'.format(TICKER), index=False)\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Keep a copy of the original file in case\n",
    "# 1231 is 2018-04-04 for whole dataset\n",
    "dataframe = dataset\n",
    "dataset = dataset.drop(['Date'], axis=1)\n",
    "\n",
    "# Generate time differences/leads as needed.\n",
    "# Now more features for time series prediction\n",
    "for i in [30]:\n",
    "    df = pd.Series(lag(dataset['Close'], -i).values.flatten(), name='Close_t+' + str(i))\n",
    "    dataset = dataset.join(df)\n",
    "\n",
    "for i in [30]:\n",
    "    df = pd.Series(difference(dataset['Close'], -i).values.flatten(), name='dClose_t+' + str(i))\n",
    "    dataset = dataset.join(df)\n",
    "\n",
    "# Perform a split\n",
    "m, n = dataset.shape\n",
    "rsplit = 0.8\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(rsplit * m)\n",
    "test_size = m - train_size\n",
    "\n",
    "# Plot the data closing\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "plt.plot(dataset['Close'].values)\n",
    "plt.title('Closing Price')\n",
    "plt.grid()\n",
    "fig.tight_layout()\n",
    "plt.savefig('price_{}.pdf'.format(TICKER), format='pdf')\n",
    "plt.show()\n",
    "\n",
    "# Drop rows with future predictions NaN\n",
    "#dataset = dataset[dataset['Close_t+30'].notnull()] <--- kills 04-04-2018 cannot be used.\n",
    "\n",
    "# Replace all NaNs in predictors with a sentinel\n",
    "# With the exception of the Close price, impute by last value\n",
    "dataset['Close_t+30'] = dataset['Close_t+30'].fillna(dataset['Close_t+30'].iloc[-30 - 1])\n",
    "dataset['dClose_t+30'] = dataset['dClose_t+30'].fillna(dataset['dClose_t+30'].iloc[-30 - 1])\n",
    "\n",
    "sentinel = -0.123456\n",
    "dataset.fillna(value=sentinel, inplace=True)\n",
    "\n",
    "train, test = dataset.iloc[0:train_size,:], dataset.iloc[train_size:m,:]\n",
    "\n",
    "# Make data look more stationary\n",
    "scaler_y = MinMaxScaler(feature_range=(0,1))\n",
    "dataset['Close_t+30'] = scaler_y.fit_transform(dataset['Close_t+30'])\n",
    "#dataset['Close_t+30'] = np.sqrt(dataset['Close_t+30'])\n",
    "\n",
    "X_train = train.drop(['Close_t+30'], axis=1)\n",
    "X_test = test.drop(['Close_t+30'], axis=1)\n",
    "y_train = train['Close_t+30']\n",
    "y_test = test['Close_t+30']\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "X_train_sc = X_train_sc.astype('float32')\n",
    "############################################################################################\n",
    "\n",
    "# Generate LSTM\n",
    "# Check this:\n",
    "# https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/ (BS)\n",
    "# https://towardsdatascience.com/time-series-analysis-in-python-an-introduction-70d5a5b1d52a\n",
    "# https://towardsdatascience.com/using-lstms-to-forecast-time-series-4ab688386b1f\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "\treturn keras.backend.sqrt(keras.backend.mean(keras.backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "mX, nX = X_train.shape\n",
    "mY,  = y_train.shape\n",
    "time_steps = 1\n",
    "batch_size = 16\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "X_train_sc = np.reshape(X_train_sc, (mX, time_steps, nX))\n",
    "X_test_sc = np.reshape(X_test_sc, (X_test_sc.shape[0], time_steps, nX))\n",
    "\n",
    "# reshape output too to be one feature.\n",
    "y_train = np.reshape(y_train, (mY, 1))\n",
    "y_test = np.reshape(y_test, (-1, 1))\n",
    "\n",
    "# Build LSTM\n",
    "# Check if model exists\n",
    "try:\n",
    "    json_file = open('model_{}.json'.format(TICKER), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights('model_{}.h5'.format(TICKER))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(input_shape=(time_steps, nX), units=nX, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1)) # no matter what, do not change this.  This is since y is a vector. \n",
    "    model.add(Activation('relu'))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    model.summary()\n",
    "    \n",
    "    model.fit(X_train_sc, y_train, epochs=2048, batch_size=batch_size, verbose=2)\n",
    "    \n",
    "    model_json = model.to_json()\n",
    "    with open('model_{}.json'.format(TICKER), 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"model_{}.h5\".format(TICKER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_orig = pd.DataFrame(y_test, columns=['Close_t+30'])#, index=test.index)\n",
    "y_test_orig = scaler_y.inverse_transform(y_test_orig)\n",
    "y_test_orig = pd.DataFrame(y_test_orig, columns=['Close_t+30'], index=test.index)\n",
    "#y_test_orig = y_test_orig ** 2\n",
    "\n",
    "y_hat = model.predict(X_test_sc, batch_size=batch_size) #** 2\n",
    "y_hat = scaler_y.inverse_transform(y_hat)\n",
    "y_hat = pd.DataFrame(y_hat.flatten(), index=test.index)\n",
    "\n",
    "# https://towardsdatascience.com/using-lstms-to-forecast-time-series-4ab688386b1f\n",
    "# Objective: green line to be 100% on the blue one after shift.\n",
    "\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "plot_actual, = plt.plot(pd.DataFrame(test['Close'].values, index=y_test_orig.index+-30), linewidth=2.75, label='True test data')\n",
    "plot_test, = plt.plot(y_test_orig, label='True lookforward-30 test data') \n",
    "plot_predicted, = plt.plot(y_hat, label='Predicted lookforward-30 data') \n",
    "plt.legend([plot_actual, plot_test, plot_predicted])\n",
    "plt.grid(True)\n",
    "#plt.xlim([0,dataset.shape[0]])\n",
    "plt.ylabel('Price in INR')\n",
    "plt.xlabel('Date')\n",
    "plt.title('Predicted Closing Price')\n",
    "fig.tight_layout()\n",
    "plt.savefig('prediction_{}.pdf'.format(TICKER), format='pdf')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################################################################    \n",
    "# This is the function that generates the requirement\n",
    "def generate_return(closing_0, closing_30):\n",
    "    return (closing_30 - closing_0) / closing_0 * 100.\n",
    "    \n",
    "############################################################################################\n",
    "\n",
    "# Now compute the return for the next 30 days\n",
    "\n",
    "y_last_date = y_hat.iloc[-1,:][0] # this is 5/16/2018 since we are predicting Close t + 30\n",
    "y_first_date = test['Close'].iloc[-1] # this is 4/4/2018\n",
    "\n",
    "generated_return = generate_return(y_first_date, y_last_date)\n",
    "\n",
    "# Conclude\n",
    "file = open('return_{}.txt'.format(TICKER),'w') \n",
    "print('For ticker {0}, the forecasted return is {1:3f}%'.format(TICKER, generated_return))\n",
    "file.write('For ticker {0}, the forecasted return is {1:3f}%'.format(TICKER, generated_return))\n",
    "\n",
    "file.close()"
   ]
  }
 ]
}